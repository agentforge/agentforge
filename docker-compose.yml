version: '3'

services:
  redis:
    image: redis:latest
    restart: always
    ports:
      - "6379:6379"
    networks:
      main-net:
        aliases:
          - redis

  # agentforge:
  #   image: agentforge
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - agentforge
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  # agentforge:
  #   image: agentforge
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - agentforge
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # llm:
  #   image: llm
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - llm
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  # llm:
  #   image: llm
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - llm
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]

  tokenizer:
    image: llm
    ipc: host
    restart: always
    networks:
      main-net:
        aliases:
          - tokenizer
    volumes:
      - cache-volume:/app/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    environment:
        - PYTHONPATH=/app/agentforge/
        - RESOURCE=tokenizer
    command: uvicorn tokenizer:app --reload --host=0.0.0.0 --port=3000
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # falcontune:
  #   image: falcontune
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - falcontune
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0']
  #             capabilities: [gpu]
  # falcontune:
  #   image: falcontune
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - falcontune
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0']
  #             capabilities: [gpu]

  # speech:
  #   image: speech
  #   restart: always
  #   ipc: host
  #   networks:
  #     main-net:
  #       aliases:
  #         - speech
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  # speech:
  #   image: speech
  #   restart: always
  #   ipc: host
  #   networks:
  #     main-net:
  #       aliases:
  #         - speech
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]

  # wav2lip:
  #   image: sadtalker
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - wav2lip
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  # wav2lip:
  #   image: sadtalker
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - wav2lip
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]

  # vqa:
  #   image: llm
  #   hostname: vqa
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - vqa
  #   volumes:
  #     - cache-volume:/app/cache
  #   environment:
  #     - RESOURCE=VQA
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  # vqa:
  #   image: llm
  #   hostname: vqa
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - vqa
  #   volumes:
  #     - cache-volume:/app/cache
  #   environment:
  #     - RESOURCE=VQA
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]

  # pixart:
  #   image: pixart
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - pixart
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  # pixart:
  #   image: pixart
  #   ipc: host
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - pixart
  #   volumes:
  #     - cache-volume:/app/cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]

  agent:
    image: agent
    environment:
      - CONFIG_DIR=""
    ports:
      - "3000:3000"
    ipc: host
    restart: always
    networks:
      main-net:
        aliases:
          - agent
    volumes:
      - cache-volume:/app/cache

  agent_dev:
    image: agent
    environment:
      - CONFIG_DIR=""
    ports:
      - "3005:3005"
    ipc: host
    restart: always
    networks:
      main-net:
        aliases:
          - agent_dev
    volumes:
      - cache-volume:/app/cache
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  vllm:
    image: vllm/vllm-openai:latest
    hostname: outlines
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0,1']
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1,2
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    expose:
      - "8000"
    networks:
      main-net:
        aliases:
          - outlines
    ipc: host
    stdin_open: true
    tty: true
    command: --model NousResearch/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0 --port 8000 --max-model-len 8192

  # outlines:
  #   image: outlines
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           device_ids: ['0,1']
  #           capabilities: [gpu]
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=1,2
  #   networks:
  #     main-net:
  #       aliases:
  #         - outlines
  #   ipc: host
  #   stdin_open: true
  #   tty: true

  # web:
  #   image: web
  #   ports:
  #     - "8080:8080"
  #   ipc: host
  #   networks:
  #     main-net:
  #       aliases:
  #         - web

  mongodb:
    image: mongo:latest
    ipc: host
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${DB_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${DB_PASS}
    ports:
      - "${DB_PORT}:27017"
    volumes:
      - db_data:/data/db
    networks:
      main-net:
        aliases:
          - mongodb

  # collector:
  #   image: collector
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - collector
  #   volumes:
  #     - cache-volume:/app/cache
  #   environment:
  #     - GOOGLE_CLOUD_CREDENTIALS=your-google-cloud-credentials
  #   tty: true
  # collector:
  #   image: collector
  #   restart: always
  #   networks:
  #     main-net:
  #       aliases:
  #         - collector
  #   volumes:
  #     - cache-volume:/app/cache
  #   environment:
  #     - GOOGLE_CLOUD_CREDENTIALS=your-google-cloud-credentials
  #   tty: true

  # etcd:
  #   container_name: milvus-etcd
  #   restart: always
  #   image: quay.io/coreos/etcd:v3.5.5
  #   environment:
  #     - ETCD_AUTO_COMPACTION_MODE=revision
  #     - ETCD_AUTO_COMPACTION_RETENTION=1000
  #     - ETCD_QUOTA_BACKEND_BYTES=4294967296
  #     - ETCD_SNAPSHOT_COUNT=50000
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/etcd:/etcd
  #   command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
  #   networks:
  #     main-net:
  #       aliases:
  #         - etcd
  # minio:
  #   container_name: milvus-minio
  #   image: minio/minio:RELEASE.2023-03-20T20-16-18Z
  #   restart: always
  #   environment:
  #     MINIO_ACCESS_KEY: minioadmin
  #     MINIO_SECRET_KEY: minioadmin
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/minio:/minio_data
  #   command: minio server /minio_data
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     main-net:
  #       aliases:
  #         - minio
  # milvus:
  #   container_name: milvus-standalone
  #   image: milvusdb/milvus:v2.2.12
  #   restart: always
  #   command: ["milvus", "run", "standalone"]
  #   environment:
  #     ETCD_ENDPOINTS: etcd:2379
  #     MINIO_ADDRESS: minio:9000
  #   networks:
  #     main-net:
  #       aliases:
  #         - milvus
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/milvus:/var/lib/milvus
  #   ports:
  #     - "19530:19530"
  #     - "9091:9091"
  #   depends_on:
  #     - "etcd"
  #     - "minio"
  # etcd:
  #   container_name: milvus-etcd
  #   restart: always
  #   image: quay.io/coreos/etcd:v3.5.5
  #   environment:
  #     - ETCD_AUTO_COMPACTION_MODE=revision
  #     - ETCD_AUTO_COMPACTION_RETENTION=1000
  #     - ETCD_QUOTA_BACKEND_BYTES=4294967296
  #     - ETCD_SNAPSHOT_COUNT=50000
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/etcd:/etcd
  #   command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
  #   networks:
  #     main-net:
  #       aliases:
  #         - etcd
  # minio:
  #   container_name: milvus-minio
  #   image: minio/minio:RELEASE.2023-03-20T20-16-18Z
  #   restart: always
  #   environment:
  #     MINIO_ACCESS_KEY: minioadmin
  #     MINIO_SECRET_KEY: minioadmin
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/minio:/minio_data
  #   command: minio server /minio_data
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     main-net:
  #       aliases:
  #         - minio
  # milvus:
  #   container_name: milvus-standalone
  #   image: milvusdb/milvus:v2.2.12
  #   restart: always
  #   command: ["milvus", "run", "standalone"]
  #   environment:
  #     ETCD_ENDPOINTS: etcd:2379
  #     MINIO_ADDRESS: minio:9000
  #   networks:
  #     main-net:
  #       aliases:
  #         - milvus
  #   volumes:
  #     - ${LOCAL_CACHE_DIR:-.}/volumes/milvus:/var/lib/milvus
  #   ports:
  #     - "19530:19530"
  #     - "9091:9091"
  #   depends_on:
  #     - "etcd"
  #     - "minio"

  # Note: If you are assigning a custom name to your db service on the line below, make sure it does not contain underscores
  logindb:
    image: 'postgres:latest'
    environment:
      POSTGRES_USER: supertokens_user 
      POSTGRES_PASSWORD: thisBeOurPostgresPassward0453 
      POSTGRES_DB: supertokens
    ports:
      - 5432:5432
    networks:
      - main-net
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'pg_isready', '-U', 'supertokens_user', '-d', 'supertokens']
      interval: 5s
      timeout: 5s
      retries: 5

  supertokens:
    image: registry.supertokens.io/supertokens/supertokens-postgresql:7.0
    depends_on:
      logindb:
        condition: service_healthy
    ports:
      - 3567:3567
    environment:
      POSTGRESQL_CONNECTION_URI: "postgresql://supertokens_user:thisBeOurPostgresPassward0453@logindb:5432/supertokens"
    networks:
      main-net:
        aliases:
          - supertokens
    restart: unless-stopped
    healthcheck:
      test: >
        bash -c 'exec 3<>/dev/tcp/127.0.0.1/3567 && echo -e "GET /hello HTTP/1.1\r\nhost: 127.0.0.1:3567\r\nConnection: close\r\n\r\n" >&3 && cat <&3 | grep "Hello"'
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  main-net:
    driver: bridge

volumes:
  db_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${LOCAL_DB_DIR}"

  cache-volume:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${LOCAL_CACHE_DIR}"
